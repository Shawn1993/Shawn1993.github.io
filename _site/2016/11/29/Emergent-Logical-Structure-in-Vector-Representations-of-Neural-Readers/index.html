<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Logical Structure in Vector Representations of Neural Readers</title>
    <link rel="shortcut icon" href="/styles/images/favicon.jpg">
    <link rel="icon" href="/styles/images/favicon.jpg">
    <link rel="stylesheet" href="/styles/css/index.css">
    <link rel="stylesheet" href="/styles/css/fontawesome/css/font-awesome.min.css">
    <link rel="canonical" href="/2016/11/29/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/">
    <link rel="alternate" type="application/rss+xml" title="Shawn's Blog" href="/feed.xml">
    <meta name="description" content="吴晓晖;个人博客">
    <!--mathjax-->
    <!--mathjax source code is here: https://github.com/mathjax/MathJax.-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]], processEscapes: true }, TeX: { equationNumbers: { autoNumber: "AMS" } }, messageStyle: "none", SVG: { blacker: 1 }});
    </script>
    <script src="/styles/js/mathjax-bind.js"></script>
    <script src="/styles/js/mathjax.js"></script>
    <!--jquery-->
    <script src="/styles/js/jquery.min.js"></script>
    <script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?94be4b0f9fc5d94cc0d0415ea6761ae9";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
    </script>
    <style type="text/css">
    .docs-content {
        margin-bottom: 10px;
    }
    </style>
</head>


  <body class="index">

    <header class="navbar navbar-inverse navbar-fixed-top docs-nav" role="banner">
    <div class="container">
        <div class="navbar-header">
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>
        <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
            <ul class="nav navbar-nav">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/categories/">分类</a>
                </li>
                <li>
                    <a href="/tag">标签</a>
                </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="disabled" href="#"><strong>不需要打赏</strong></a>
                </li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">关于<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a rel="nofollow" href="/books">我的书单</a></li>
                        <li><a rel="nofollow" href="/reference">推荐博客</a></li>
                        <li><a rel="nofollow" target="_blank" href="https://github.com/Shawn1993">Github</a></li>
                        <li><a rel="nofollow" target="_blank" href="https://Shawn1993.github.io">About</a></li>
                        <li class="divider"></li>
                        <li><a rel="nofollow" target="_blank" href="https://github.com/luoyan35714/LessOrMore.git">博客项目</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </div>
</header>

    <div class="docs-header" id="content">
  <div class="container">
	    <h1>Shawn's Blog</h1>
	    <p>非一日之寒</p>
  </div>
</div>
    
      
<div class="banner">
  <div class="container">
  	
    	<a href="/categories/#论文阅读-ref">论文阅读</a>	/
    	<a href="/tag/#QA-ref">QA</a>
    
  </div>
</div>

    

    <div class="container docs-container">
    <div class="row">
        <div class="col-md-3">
            <div class="sidebar hidden-print" role="complementary">
                <div id="navigation">
  <h1>目录</h1>
  <ul class="nav sidenav">
<!--
    
      
      
      
      

      
        <li><a href="#year_2016">2016</a>
          <ul class="nav">
            <li><a href="#month_2016_December">December</a></li>
      

      
            
          
              <li><a href="#month_2016_November">November</a></li>
          
        
      
    
      
      
      
      

      

      
            </ul>
          </li>
      
    
-->
  </ul>
</div>
            </div>
        </div>
        <div class="col-md-9" role="main">
            <div class="panel docs-content">
                <div class="wrapper">
                    <header class="post-header">
                        <h1 class="post-title">Emergent Logical Structure in Vector Representations of Neural Readers</h1>
                        <!--
                <p class="post-meta">Nov 29, 2016</p>
              -->
                        <div class="meta">Posted on <span class="postdate">Nov 29, 2016</span> By <a target="_blank" href="shawn1993.github.io">Shawn</a></div>
                        <br />
                    </header>
                    <article class="post-content">
                        <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">一、文章信息</a>    <ul>
      <li><a href="#section-1" id="markdown-toc-section-1">作者</a></li>
      <li><a href="#section-2" id="markdown-toc-section-2">单位</a></li>
      <li><a href="#section-3" id="markdown-toc-section-3">文章来源</a></li>
    </ul>
  </li>
  <li><a href="#section-4" id="markdown-toc-section-4">二、文章内容</a>    <ul>
      <li><a href="#section-5" id="markdown-toc-section-5">1 解决问题</a></li>
      <li><a href="#aggregation-readers" id="markdown-toc-aggregation-readers">2 Aggregation Readers模型</a>        <ul>
          <li><a href="#stanford-reader" id="markdown-toc-stanford-reader">2.1 Stanford Reader</a></li>
          <li><a href="#memory-network" id="markdown-toc-memory-network">2.2 Memory Network</a></li>
          <li><a href="#attentive-reader" id="markdown-toc-attentive-reader">2.3 Attentive Reader</a></li>
        </ul>
      </li>
      <li><a href="#explicit-reference-readers" id="markdown-toc-explicit-reference-readers">3 Explicit Reference Readers模型</a>        <ul>
          <li><a href="#attention-sum-reader" id="markdown-toc-attention-sum-reader">3.1 Attention-Sum Reader</a></li>
          <li><a href="#gated-attention-reader" id="markdown-toc-gated-attention-reader">3.2 Gated-Attention Reader</a></li>
          <li><a href="#attenion-over-attention-reader" id="markdown-toc-attenion-over-attention-reader">3.3 Attenion-over-Attention Reader</a></li>
        </ul>
      </li>
      <li><a href="#readers" id="markdown-toc-readers">4 两种readers的相似性</a></li>
      <li><a href="#section-6" id="markdown-toc-section-6">5 数据集</a></li>
      <li><a href="#section-7" id="markdown-toc-section-7">6 相关论文</a></li>
    </ul>
  </li>
  <li><a href="#section-8" id="markdown-toc-section-8">三、简评</a></li>
</ul>

<h2 id="section">一、文章信息</h2>

<h3 id="section-1">作者</h3>
<p>Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester</p>

<h3 id="section-2">单位</h3>
<p>Toyota Technological Institute at Chicago</p>

<h3 id="section-3">文章来源</h3>
<p><a href="https://arxiv.org/abs/1611.07954">ICLR 2017 Submission</a></p>

<h2 id="section-4">二、文章内容</h2>

<h3 id="section-5">1 解决问题</h3>
<p>本文中作者认为最近提出的各种attention based readers 可以分为两类，进行了全面的总结，并且作者从数学层面分析了两类Reader的相关性。</p>

<h3 id="aggregation-readers">2 Aggregation Readers模型</h3>
<p>这种Readers是最先出现的，包括Memory Networks，Attentive Reader，Stanford Reader等</p>

<h4 id="stanford-reader">2.1 Stanford Reader</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p))\tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\end{align} %]]></script>

<p>$e(p)$表示paragraph的词向量$e(w_i)$序列，$w_i \in p$。<br />
$e(q)$表示question的词向量序列。<br />
biLSTM(s)表示双向LSTM的hidden state序列。<br />
fLSTM(s)，bLSTM(s)分别表示前向LSTM和后向LSTM的hidden stae序列。<br />
$[\cdot,\cdot]$表示concatenation。</p>

<p>接下来的可以看作是Attention机制：<br />
<script type="math/tex">% <![CDATA[
\begin{align}
\alpha_t &=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
\end{align} %]]></script></p>

<p>a表示答案，q表示问题，p表示段落线索，$\mathcal{A}$表示问题候选集。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
P(a\mid p,q,\mathcal{A}) &= \mathop{softmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{6}
\end{align} %]]></script>

<p>$e_o(a)$表示问题的output embedding，$e$和$e_o$属于不同的向量空间。</p>

<h4 id="memory-network">2.2 Memory Network</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p))\tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]></script>

<p>跟Stanford Reader的区别就是最后求概率时，是求词表大小的概率分布，所以它将训练整个词表的output embedding。</p>

<h4 id="attentive-reader">2.3 Attentive Reader</h4>
<p>Stanford Reader就是从Attentive Reader得来的。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p)) \tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t \text{MLP}([h_t,q])\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^T\text{MLP}([o,q])\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]></script>

<p>MLP是指多层感知器(multi layer perceptron)。在词表上预测结果，会使得在非匿名的数据集上的表现提高。</p>

<h3 id="explicit-reference-readers">3 Explicit Reference Readers模型</h3>

<h4 id="attention-sum-reader">3.1 Attention-Sum Reader</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h&=\text{biGRU}(e(p))\tag{1}\\
q&=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t h_t^Tq\tag{3}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t\tag{4}\\
\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{5}
\end{align} %]]></script>

<h4 id="gated-attention-reader">3.2 Gated-Attention Reader</h4>
<p>它有多层双向GRU。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
q^\ell&=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\ 1\leq\ell\leq K \tag{1}\\
h^1&=\text{biGRU}(e(p))\tag{2}\\
h^\ell&= \text{biGRU}(h^{\ell-1}\odot q^{\ell-1})2\leq\ell\leq K\tag{3}\\
\alpha_t &=\mathop{softmax}\limits_t (h_t^K)^Tq^K\tag{4}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{6}
\end{align} %]]></script>

<h4 id="attenion-over-attention-reader">3.3 Attenion-over-Attention Reader</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
h & = \text{biGRU}(e(p)) &q&=\text{biGRU}(e(q))\\
\alpha_{t,j} &= softmax_t\ h_t^Tq_j &\beta_{t,j}  &= softmax_j h_t^Tq^j\\
\beta_j&=\frac{1}{|p|}\sum_t\beta_{t,j} &\alpha_t&=\sum_j\beta_j\alpha_{t,j}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t &\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t
\end{align*} %]]></script>

<p>使用更精密的方法计算attention，$q_j$表示双向GRU的hiddent state序列中的第$j$个向量。</p>

<h3 id="readers">4 两种readers的相似性</h3>
<p>aggregation readers在匿名化的数据中表现的也不错，所以我们猜想aggregation readers中的$o$包含了一定的pointer信息，作者认为$h_t$和$e_o(a)$有以下关系：</p>

<script type="math/tex; mode=display">e_o(a)^Th_t = \left\{\begin{array}{l}
\ c\ \ \text{if}\ t\in R(a,p)\\
\ 0\ \ \text{otherwise}
\end{array}\right.</script>

<p>如果满足上述关系就会推出两种readers是等价的：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathop{\text{argmax}}_a e_0(a)^To &= \mathop{\text{argmax}}_a e_o(a)^T\sum_t \alpha_th_t\\
&=\mathop{\text{argmax}}_a\sum_t\alpha_te_o(a)^Th_t = \mathop{\text{argmax}}_a \sum_{t\in R(a,p)}\alpha_t
\end{align*} %]]></script>

<p>同时作者也用数据来证明了这种猜想：<br />
<img src="http://oddpnmpll.bkt.clouddn.com/2016-11-29-085545.jpg" alt="" /></p>

<p>作者还认为attention $\alpha_t$和匿名化后的ID顺序无关，两个具有不同ID顺序的相同文档，应该具有相同的attention，$q^Th_t=q^Th_t^{‘}$。因此认为$h_t$包含与ID相关的内容，也包含与ID无关的内容：</p>

<script type="math/tex; mode=display">q^T(h_i+e_o(a))=q^Th_{i\cdot}</script>

<p>也就是等价于$q^Te_o(a)=0$，同时作者也用数据来证明了：<br />
<img src="http://oddpnmpll.bkt.clouddn.com/2016-11-29-091617.jpg" alt="" /></p>

<h3 id="section-6">5 数据集</h3>
<ol>
  <li>
    <p><a href="http://cs.nyu.edu/~kcho/DMQA/">CNN &amp; DailyMail</a>  <br />
论文：<a href="https://arxiv.org/abs/1506.03340">Teaching Machines to Read and Comprehend</a>  <br />
数据来自CNN和Daily Mail新闻，文章中高亮显示而且挖空的就是问题。为了防止使用外界知识，将命名实体都用ID替换，给出答案候选集。</p>
  </li>
  <li>
    <p><a href="https://tticnlp.github.io/who_did_what/">Who-did-What(WDW)</a>  <br />
论文：<a href="http://aclweb.org/anthology/D/D16/D16-1241.pdf">Who did What: A Large-Scale Person-Centered Cloze Dataset</a>  <br />
数据来自LDC English Gigaword newswire corpus。该数据集为了防止文章摘要被使用，每一个问题都从两个独立的文章中生成，一篇用来做Context，一篇用来挖空作为问题。该数据集为了不像CNN&amp;DailyMail那样将实体匿名，所有的问题都是人名实体。而且使用了一些简单的baselines来筛选掉那些容易解决的问题。</p>
  </li>
  <li>
    <p>Children’s Book Test(CBT)  <br />
论文：<a href="https://arxiv.org/abs/1511.02301">The goldilocks principle: Reading childrens books with explicit memory representations</a>  <br />
数据来自一个儿童读物，每个问题都是从中挑选出21条连续的句子，将前20条作为Context，将第21条挖空作为问题。</p>
  </li>
  <li>
    <p><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a>  <br />
论文：<a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></p>
  </li>
  <li>
    <p><a href="http://fb.ai/babi">bAbI</a>  <br />
论文：<a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></p>
  </li>
</ol>

<h3 id="section-7">6 相关论文</h3>
<ol>
  <li>Stanford Reader  <br />
<a href="https://arxiv.org/abs/1606.02858">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a></li>
  <li>Memory Networks  <br />
<a href="https://arxiv.org/abs/1503.08895">End-To-End Memory Networks</a></li>
  <li>Attentive Networks  <br />
<a href="https://arxiv.org/abs/1506.03340">Teaching Machines to Read and Comprehend</a></li>
  <li>Attention-Sum Reader  <br />
<a href="https://arxiv.org/abs/1603.01547">Text Understanding with the Attention Sum Reader Network</a></li>
  <li>Gated-Attention Reader  <br />
<a href="https://arxiv.org/abs/1606.01549">Gated-Attention Readers for Text Comprehension</a></li>
  <li>Attention-over-Attention Reader  <br />
<a href="https://arxiv.org/abs/1607.04423">Attention-over-Attention Neural Networks for Reading Comprehension</a></li>
</ol>

<h2 id="section-8">三、简评</h2>
<p>在我看来这是一篇很全面的综述，作者全面总结了最近出现的各种Readers，对开展机器阅读方面的研究有一个很好的参考。但是我很好奇为什么这里没有提到Dynamic Memory Networks，但是我觉得不好归类吧，毕竟Dynamic Memory Networks的Answers是通过RNN来decode而得来的。</p>


                    </article>
                </div>
            </div>
            <div class="panel docs-content">
                <div class="post-content">
                    <div class="wrapper text-center" style="padding: 20px">
                        暂无评论功能
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

    
    <footer class="footer" role="contentinfo">
	<div class="container">
		<p class="copyright">Copyright &copy; 2014-2017 <a href="https://Shawn1993.github.io"><code>Shawn</code></a>.</p>
		<p>Powered by <a href="http://jekyllrb.com">Jekyll</a>, theme from <a href="http://lesscss.cn/">Less</a></p>
	</div>
</footer>

<script src="/styles/js/jquery.min.js"></script>
<script src="/styles/js/bootstrap.min.js"></script>
<script src="/styles/js/holder.min.js"></script>
<script src="/styles/js/application.js"></script>
<script src="/styles/js/lessismore.js"></script>

  </body>
</html>
