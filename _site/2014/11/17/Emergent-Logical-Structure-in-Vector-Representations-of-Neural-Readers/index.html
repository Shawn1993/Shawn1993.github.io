<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Welcome to Jekyll!</title>

	<link rel="shortcut icon" href="/styles/images/favicon.jpg">
	<link rel="icon" href="/styles/images/favicon.jpg">

	<link rel="stylesheet" href="/styles/css/index.css">
	<link rel="stylesheet" href="/styles/css/fontawesome/css/font-awesome.min.css">
	<link rel="canonical" href="/2014/11/17/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/">
	<link rel="alternate" type="application/rss+xml" title="Shawn's Blog" href="/feed.xml">
	
	<meta name="description" content="吴晓晖;个人博客">

	<script src="/styles/js/jquery.min.js"></script>
	<!--[if lt IE 9]>
    	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  	<![endif]-->
  	<script>
		var _hmt = _hmt || [];
		(function() {
		  var hm = document.createElement("script");
		  hm.src = "//hm.baidu.com/hm.js?94be4b0f9fc5d94cc0d0415ea6761ae9";
		  var s = document.getElementsByTagName("script")[0]; 
		  s.parentNode.insertBefore(hm, s);
		})();
	</script>
  	<style type="text/css">
	  	.docs-content{
	  		margin-bottom: 10px;
	  	}
  	</style>
</head>

  <body class="index">

    <header class="navbar navbar-inverse navbar-fixed-top docs-nav" role="banner">
    <div class="container">
        <div class="navbar-header">
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>
        <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
            <ul class="nav navbar-nav">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/categories/">目录</a>
                </li>
                <li>
                    <a href="/tag">标签</a>
                </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/donate/"><strong>打赏</strong></a>
                </li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">关于<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a rel="nofollow" target="_blank" href="https://github.com/Shawn1993">Github</a></li>
                        <li><a rel="nofollow" target="_blank" href="https://Shawn1993.github.io">关于作者</a></li>
                        <li><a rel="nofollow" href="/books">我的书单</a></li>
                        <li><a rel="nofollow" href="/reference">推荐博客</a></li>
                        <li><a href="/feed.xml">RSS订阅</a></li>
                        <li class="divider"></li>
                        <li><a rel="nofollow" target="_blank" href="https://github.com/luoyan35714/LessOrMore.git">本项目</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </div>
</header>

    <div class="docs-header" id="content">
  <div class="container">
  	
  		<!--
		    <h1>Welcome to Jekyll!</h1>
		    <p>Post on Nov 17, 2014 by <a href="/about">Shawn</a></p>
		-->
		    <h1>没啥</h1>
    
  </div>
</div>
    
      
<div class="banner">
  <div class="container">
  	
    	<a href="/categories/#论文阅读-ref">论文阅读</a>	/
    	<a href="/tag/#QA-ref">QA</a>
    
  </div>
</div>

    

    <div class="container docs-container">
  <div class="row">
    <div class="col-md-3">
      <div class="sidebar hidden-print" role="complementary">
        <div id="navigation">
  <h1>目录</h1>
  <ul class="nav sidenav">
<!--
    
      
      
      
      

      
        <li><a href="#year_2016">2016</a>
          <ul class="nav">
            <li><a href="#month_2016_August">August</a></li>
      

      
        
            </ul>
          </li>
          <li><a href="#year_2014">2014</a>
            <ul class="nav">
              <li><a href="#month_2014_December">December</a></li>
        
      
    
      
      
      
      

      

      
            
          
              <li><a href="#month_2014_November">November</a></li>
          
        
      
    
      
      
      
      

      

      
            
          
        
      
    
      
      
      
      

      

      
            </ul>
          </li>
      
    
-->
  </ul>
</div> 
      </div>
    </div>
    <div class="col-md-9" role="main">
      <div class="panel docs-content">
        <div class="wrapper">
            <header class="post-header">
              <h1 class="post-title">Welcome to Jekyll!</h1>
              <!--
                <p class="post-meta">Nov 17, 2014</p>
              -->
              <div class="meta">Posted on <span class="postdate">Nov 17, 2014</span> By <a target="_blank" href="https://Shawn1993.github.io">Shawn</a></div>
              <br />
            </header>
            <article class="post-content">
              <ul id="markdown-toc">
  <li><a href="#emergent-logical-structure-in-vector-representations-of-neural-readers" id="markdown-toc-emergent-logical-structure-in-vector-representations-of-neural-readers">Emergent Logical Structure in Vector Representations of Neural Readers</a>    <ul>
      <li><a href="#section" id="markdown-toc-section">一、文章信息</a></li>
      <li><a href="#section-1" id="markdown-toc-section-1">二、文章内容</a>        <ul>
          <li><a href="#stanford-reader" id="markdown-toc-stanford-reader">2.1 Stanford Reader</a></li>
          <li><a href="#memory-network" id="markdown-toc-memory-network">2.2 Memory Network</a></li>
          <li><a href="#attentive-reader" id="markdown-toc-attentive-reader">2.3 Attentive Reader</a></li>
          <li><a href="#explicit-reference-readers" id="markdown-toc-explicit-reference-readers">3 Explicit Reference Readers模型</a>            <ul>
              <li><a href="#attention-sum-reader" id="markdown-toc-attention-sum-reader">3.1 Attention-Sum Reader</a></li>
              <li><a href="#gated-attention-reader" id="markdown-toc-gated-attention-reader">3.2 Gated-Attention Reader</a></li>
              <li><a href="#attenion-over-attention-reader" id="markdown-toc-attenion-over-attention-reader">3.3 Attenion-over-Attention Reader</a></li>
            </ul>
          </li>
          <li><a href="#readers" id="markdown-toc-readers">4 两种readers的相似性</a></li>
          <li><a href="#section-2" id="markdown-toc-section-2">5 数据集</a></li>
          <li><a href="#section-3" id="markdown-toc-section-3">6 相关论文</a></li>
        </ul>
      </li>
      <li><a href="#section-4" id="markdown-toc-section-4">三、简评</a></li>
    </ul>
  </li>
</ul>

<h1 id="emergent-logical-structure-in-vector-representations-of-neural-readers">Emergent Logical Structure in Vector Representations of Neural Readers</h1>

<h2 id="section">一、文章信息</h2>
<p>### 作者
Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester
### 单位
Toyota Technological Institute at Chicago
### 文章来源
<a href="https://arxiv.org/abs/1611.07954">ICLR 2017 Submission</a></p>

<h2 id="section-1">二、文章内容</h2>
<p>### 1 解决问题
本文中作者认为最近提出的各种attention based readers 可以分为两类，进行了全面的总结，并且作者从数学层面分析了两类Reader的相关性。
### 2 Aggregation Readers模型
这种Readers是最先出现的，包括Memory Networks，Attentive Reader，Stanford Reader等</p>

<h4 id="stanford-reader">2.1 Stanford Reader</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p))\tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\end{align} %]]></script>

<p>$e(p)$表示paragraph的词向量$e(w_i)$序列，$w_i \in p$。
$e(q)$表示question的词向量序列。
biLSTM(s)表示双向LSTM的hidden state序列。
fLSTM(s)，bLSTM(s)分别表示前向LSTM和后向LSTM的hidden stae序列。
$[\cdot,\cdot]$表示concatenation。</p>

<p>接下来的可以看作是Attention机制：
<script type="math/tex">% <![CDATA[
\begin{align}
\alpha_t &=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
\end{align} %]]></script></p>

<p>a表示答案，q表示问题，p表示段落线索，$\mathcal{A}$表示问题候选集。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
P(a\mid p,q,\mathcal{A}) &= \mathop{softmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{6}
\end{align} %]]></script>

<p>$e_o(a)$表示问题的output embedding，$e$和$e_o$属于不同的向量空间。</p>

<h4 id="memory-network">2.2 Memory Network</h4>
<p><script type="math/tex">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p))\tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]></script>
跟Stanford Reader的区别就是最后求概率时，是求词表大小的概率分布，所以它将训练整个词表的output embedding。</p>

<h4 id="attentive-reader">2.3 Attentive Reader</h4>
<p>Stanford Reader就是从Attentive Reader得来的。
<script type="math/tex">% <![CDATA[
\begin{align}
h&=\text{biLSTM}(e(p)) \tag{1}\\
q&=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t \text{MLP}([h_t,q])\tag{3}\\
o &= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^T\text{MLP}([o,q])\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]></script>
MLP是指多层感知器(multi layer perceptron)。在词表上预测结果，会使得在非匿名的数据集上的表现提高。</p>

<h3 id="explicit-reference-readers">3 Explicit Reference Readers模型</h3>

<h4 id="attention-sum-reader">3.1 Attention-Sum Reader</h4>
<p><script type="math/tex">% <![CDATA[
\begin{align}
h&=\text{biGRU}(e(p))\tag{1}\\
q&=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\tag{2}\\
\alpha_t &=\mathop{softmax}\limits_t h_t^Tq\tag{3}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t\tag{4}\\
\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{5}
\end{align} %]]></script></p>

<h4 id="gated-attention-reader">3.2 Gated-Attention Reader</h4>
<p>它有多层双向GRU。
<script type="math/tex">% <![CDATA[
\begin{align}
q^\ell&=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\ 1\leq\ell\leq K \tag{1}\\
h^1&=\text{biGRU}(e(p))\tag{2}\\
h^\ell&= \text{biGRU}(h^{\ell-1}\odot q^{\ell-1})2\leq\ell\leq K\tag{3}\\
\alpha_t &=\mathop{softmax}\limits_t (h_t^K)^Tq^K\tag{4}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t\tag{5}\\
\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{6}
\end{align} %]]></script></p>

<h4 id="attenion-over-attention-reader">3.3 Attenion-over-Attention Reader</h4>
<p><script type="math/tex">% <![CDATA[
\begin{align*}
h & = \text{biGRU}(e(p)) &q&=\text{biGRU}(e(q))\\
\alpha_{t,j} &= softmax_t\ h_t^Tq_j &\beta_{t,j}  &= softmax_j h_t^Tq^j\\
\beta_j&=\frac{1}{|p|}\sum_t\beta_{t,j} &\alpha_t&=\sum_j\beta_j\alpha_{t,j}\\
P(a\mid p,q,\mathcal{A}) &= \sum_{t\in R(a,p)}\alpha_t &\hat{a} &=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t
\end{align*} %]]></script></p>

<p>使用更精密的方法计算attention，$q_j$表示双向GRU的hiddent state序列中的第$j$个向量。</p>

<h3 id="readers">4 两种readers的相似性</h3>
<p>aggregation readers在匿名化的数据中表现的也不错，所以我们猜想aggregation readers中的$o$包含了一定的pointer信息，作者认为$h_t$和$e_o(a)$有以下关系：
<script type="math/tex">e_o(a)^Th_t = \left\{\begin{array}{l}
\ c\ \ \text{if}\ t\in R(a,p)\\
\ 0\ \ \text{otherwise}
\end{array}\right.</script>
如果满足上述关系就会推出两种readers是等价的：
<script type="math/tex">% <![CDATA[
\begin{align*}
\mathop{\text{argmax}}_a e_0(a)^To &= \mathop{\text{argmax}}_a e_o(a)^T\sum_t \alpha_th_t\\
&=\mathop{\text{argmax}}_a\sum_t\alpha_te_o(a)^Th_t = \mathop{\text{argmax}}_a \sum_{t\in R(a,p)}\alpha_t
\end{align*} %]]></script>
同时作者也用数据来证明了这种猜想：
<img src="http://oddpnmpll.bkt.clouddn.com/2016-11-29-085545.jpg" alt="" /></p>

<p>作者还认为attention $\alpha_t$和匿名化后的ID顺序无关，两个具有不同ID顺序的相同文档，应该具有相同的attention，$q^Th_t=q^Th_t^{‘}$。因此认为$h_t$包含与ID相关的内容，也包含与ID无关的内容：
<script type="math/tex">q^T(h_i+e_o(a))=q^Th_{i\cdot}</script></p>

<p>也就是等价于$q^Te_o(a)=0$，同时作者也用数据来证明了：
<img src="http://oddpnmpll.bkt.clouddn.com/2016-11-29-091617.jpg" alt="" /></p>

<h3 id="section-2">5 数据集</h3>
<ol>
  <li>
    <p><a href="http://cs.nyu.edu/~kcho/DMQA/">CNN &amp; DailyMail</a>
论文：<a href="https://arxiv.org/abs/1506.03340">Teaching Machines to Read and Comprehend</a><br />
数据来自CNN和Daily Mail新闻，文章中高亮显示而且挖空的就是问题。为了防止使用外界知识，将命名实体都用ID替换，给出答案候选集。</p>
  </li>
  <li>
    <p><a href="https://tticnlp.github.io/who_did_what/">Who-did-What(WDW)</a>
论文：<a href="http://aclweb.org/anthology/D/D16/D16-1241.pdf">Who did What: A Large-Scale Person-Centered Cloze Dataset</a><br />
数据来自LDC English Gigaword newswire copus。该数据集为了防止文章摘要被使用，每一个问题都从两个独立的文章中生成，一篇用来做Context，一篇用来挖空作为问题。该数据集为了不像CNN&amp;DailyMail那样将实体匿名，所有的问题都是人名实体。而且使用了一些简单的baselines来筛选掉那些容易解决的问题。</p>
  </li>
  <li>
    <p>Children’s Book Test(CBT)
论文：<a href="https://arxiv.org/abs/1511.02301">The goldilocks principle: Reading childrens books with explicit memory representations</a><br />
数据来自一个儿童读物，每个问题都是从中挑选出21条连续的句子，将前20条作为Context，将第21条挖空作为问题。</p>
  </li>
  <li>
    <p><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a> 
论文：<a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></p>
  </li>
  <li>
    <p><a href="http://fb.ai/babi">bAbI</a>
论文：<a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></p>
  </li>
</ol>

<h3 id="section-3">6 相关论文</h3>
<ol>
  <li>Stanford Reader
<a href="https://arxiv.org/abs/1606.02858">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a></li>
  <li>Memory Networks
<a href="https://arxiv.org/abs/1503.08895">End-To-End Memory Networks</a></li>
  <li>Attentive Networks
<a href="https://arxiv.org/abs/1506.03340">Teaching Machines to Read and Comprehend</a></li>
  <li>Attention-Sum Reader
<a href="https://arxiv.org/abs/1603.01547">Text Understanding with the Attention Sum Reader Network</a></li>
  <li>Gated-Attention Reader
<a href="https://arxiv.org/abs/1606.01549">Gated-Attention Readers for Text Comprehension</a></li>
  <li>Attention-over-Attention Reader
<a href="https://arxiv.org/abs/1607.04423">Attention-over-Attention Neural Networks for Reading Comprehension</a></li>
</ol>

<h2 id="section-4">三、简评</h2>
<p>在我看来这是一篇很全面的综述，作者全面总结了最近出现的各种Readers，对开展机器阅读方面的研究有一个很好的参考。但是我很好奇为什么这里没有提到Dynamic Memory Networks，但是我觉得不好归类吧，毕竟Dynamic Memory Networks的Answers是通过RNN来decode而得来的。</p>


            </article>
        </div>
      </div>
      <div class="panel docs-content">
        <article class="post-content">
          <div class="wrapper">
            


  <div class="ds-thread" data-thread-key="/2014/11/17/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/" data-title="Welcome to Jekyll!" data-url="https://Shawn1993.github.io/2014/11/17/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/"></div>

<script type="text/javascript">
var duoshuoQuery = {short_name:"luoyan35714"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
</script>


 
          </div>
        </article>
      </div>
    </div>
  </div>
</div>

    
    <footer class="footer" role="contentinfo">
	<div class="container">
		<p class="copyright">Copyright &copy; 2014-2016 <a href="https://Shawn1993.github.io"><code>Shawn</code></a>.</p>
		<p>Powered by <a href="http://jekyllrb.com">Jekyll</a>, theme from <a href="http://lesscss.cn/">Less</a></p>
	</div>
</footer>

<script src="/styles/js/jquery.min.js"></script>
<script src="/styles/js/bootstrap.min.js"></script>
<script src="/styles/js/holder.min.js"></script>
<script src="/styles/js/application.js"></script>
<script src="/styles/js/lessismore.js"></script>

  </body>
</html>
