<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>非一日之寒</description>
    <link>https://Shawn1993.github.io/</link>
    <atom:link href="https://Shawn1993.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 30 Nov 2016 22:23:51 +0800</pubDate>
    <lastBuildDate>Wed, 30 Nov 2016 22:23:51 +0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Emergent Logical Structure in Vector Representations of Neural Readers</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;一、文章信息&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;作者&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;单位&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;文章来源&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;二、文章内容&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;1 解决问题&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#aggregation-readers&quot; id=&quot;markdown-toc-aggregation-readers&quot;&gt;2 Aggregation Readers模型&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#stanford-reader&quot; id=&quot;markdown-toc-stanford-reader&quot;&gt;2.1 Stanford Reader&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#memory-network&quot; id=&quot;markdown-toc-memory-network&quot;&gt;2.2 Memory Network&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#attentive-reader&quot; id=&quot;markdown-toc-attentive-reader&quot;&gt;2.3 Attentive Reader&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#explicit-reference-readers&quot; id=&quot;markdown-toc-explicit-reference-readers&quot;&gt;3 Explicit Reference Readers模型&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#attention-sum-reader&quot; id=&quot;markdown-toc-attention-sum-reader&quot;&gt;3.1 Attention-Sum Reader&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#gated-attention-reader&quot; id=&quot;markdown-toc-gated-attention-reader&quot;&gt;3.2 Gated-Attention Reader&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#attenion-over-attention-reader&quot; id=&quot;markdown-toc-attenion-over-attention-reader&quot;&gt;3.3 Attenion-over-Attention Reader&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#readers&quot; id=&quot;markdown-toc-readers&quot;&gt;4 两种readers的相似性&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;5 数据集&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;6 相关论文&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;三、简评&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;一、文章信息&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;作者&lt;/h3&gt;
&lt;p&gt;Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;单位&lt;/h3&gt;
&lt;p&gt;Toyota Technological Institute at Chicago&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;文章来源&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.07954&quot;&gt;ICLR 2017 Submission&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;二、文章内容&lt;/h2&gt;

&lt;h3 id=&quot;section-5&quot;&gt;1 解决问题&lt;/h3&gt;
&lt;p&gt;本文中作者认为最近提出的各种attention based readers 可以分为两类，进行了全面的总结，并且作者从数学层面分析了两类Reader的相关性。&lt;/p&gt;

&lt;h3 id=&quot;aggregation-readers&quot;&gt;2 Aggregation Readers模型&lt;/h3&gt;
&lt;p&gt;这种Readers是最先出现的，包括Memory Networks，Attentive Reader，Stanford Reader等&lt;/p&gt;

&lt;h4 id=&quot;stanford-reader&quot;&gt;2.1 Stanford Reader&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h&amp;=\text{biLSTM}(e(p))\tag{1}\\
q&amp;=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$e(p)$表示paragraph的词向量$e(w_i)$序列，$w_i \in p$。&lt;br /&gt;
$e(q)$表示question的词向量序列。&lt;br /&gt;
biLSTM(s)表示双向LSTM的hidden state序列。&lt;br /&gt;
fLSTM(s)，bLSTM(s)分别表示前向LSTM和后向LSTM的hidden stae序列。&lt;br /&gt;
$[\cdot,\cdot]$表示concatenation。&lt;/p&gt;

&lt;p&gt;接下来的可以看作是Attention机制：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\alpha_t &amp;=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &amp;= \sum_t \alpha_t h_t\tag{4}\\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;a表示答案，q表示问题，p表示段落线索，$\mathcal{A}$表示问题候选集。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
P(a\mid p,q,\mathcal{A}) &amp;= \mathop{softmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{5}\\
\hat{a} &amp;=\mathop{argmax}\limits_{a\in\mathcal{A}}\ e_o(a)^To\tag{6}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$e_o(a)$表示问题的output embedding，$e$和$e_o$属于不同的向量空间。&lt;/p&gt;

&lt;h4 id=&quot;memory-network&quot;&gt;2.2 Memory Network&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h&amp;=\text{biLSTM}(e(p))\tag{1}\\
q&amp;=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &amp;=\mathop{softmax}\limits_t\ h_t^TW_{\alpha}q\tag{3}\\
o &amp;= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &amp;=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{5}\\
\hat{a} &amp;=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;跟Stanford Reader的区别就是最后求概率时，是求词表大小的概率分布，所以它将训练整个词表的output embedding。&lt;/p&gt;

&lt;h4 id=&quot;attentive-reader&quot;&gt;2.3 Attentive Reader&lt;/h4&gt;
&lt;p&gt;Stanford Reader就是从Attentive Reader得来的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h&amp;=\text{biLSTM}(e(p)) \tag{1}\\
q&amp;=[\text{fLSTM}(e(q))_{|q|},bLSTM(e(q))_1]\tag{2}\\
\alpha_t &amp;=\mathop{softmax}\limits_t \text{MLP}([h_t,q])\tag{3}\\
o &amp;= \sum_t \alpha_t h_t\tag{4}\\
P(w\mid p,q,\mathcal{A}) &amp;=P(w\mid p,q)= \mathop{softmax}\limits_{w\in\mathcal{V}}\ e_o(w)^T\text{MLP}([o,q])\tag{5}\\
\hat{a} &amp;=\mathop{argmax}\limits_{w\in\mathcal{V}}\ e_o(w)^To\tag{6}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;MLP是指多层感知器(multi layer perceptron)。在词表上预测结果，会使得在非匿名的数据集上的表现提高。&lt;/p&gt;

&lt;h3 id=&quot;explicit-reference-readers&quot;&gt;3 Explicit Reference Readers模型&lt;/h3&gt;

&lt;h4 id=&quot;attention-sum-reader&quot;&gt;3.1 Attention-Sum Reader&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h&amp;=\text{biGRU}(e(p))\tag{1}\\
q&amp;=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\tag{2}\\
\alpha_t &amp;=\mathop{softmax}\limits_t h_t^Tq\tag{3}\\
P(a\mid p,q,\mathcal{A}) &amp;= \sum_{t\in R(a,p)}\alpha_t\tag{4}\\
\hat{a} &amp;=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{5}
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;gated-attention-reader&quot;&gt;3.2 Gated-Attention Reader&lt;/h4&gt;
&lt;p&gt;它有多层双向GRU。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
q^\ell&amp;=[\text{fGRU}(e(q))_{|q|},bGRU(e(q))_1]\ 1\leq\ell\leq K \tag{1}\\
h^1&amp;=\text{biGRU}(e(p))\tag{2}\\
h^\ell&amp;= \text{biGRU}(h^{\ell-1}\odot q^{\ell-1})2\leq\ell\leq K\tag{3}\\
\alpha_t &amp;=\mathop{softmax}\limits_t (h_t^K)^Tq^K\tag{4}\\
P(a\mid p,q,\mathcal{A}) &amp;= \sum_{t\in R(a,p)}\alpha_t\tag{5}\\
\hat{a} &amp;=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t\tag{6}
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;attenion-over-attention-reader&quot;&gt;3.3 Attenion-over-Attention Reader&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
h &amp; = \text{biGRU}(e(p)) &amp;q&amp;=\text{biGRU}(e(q))\\
\alpha_{t,j} &amp;= softmax_t\ h_t^Tq_j &amp;\beta_{t,j}  &amp;= softmax_j h_t^Tq^j\\
\beta_j&amp;=\frac{1}{|p|}\sum_t\beta_{t,j} &amp;\alpha_t&amp;=\sum_j\beta_j\alpha_{t,j}\\
P(a\mid p,q,\mathcal{A}) &amp;= \sum_{t\in R(a,p)}\alpha_t &amp;\hat{a} &amp;=\mathop{argmax}\limits_{a}\sum_{t\in R(a,p)} \alpha_t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;使用更精密的方法计算attention，$q_j$表示双向GRU的hiddent state序列中的第$j$个向量。&lt;/p&gt;

&lt;h3 id=&quot;readers&quot;&gt;4 两种readers的相似性&lt;/h3&gt;
&lt;p&gt;aggregation readers在匿名化的数据中表现的也不错，所以我们猜想aggregation readers中的$o$包含了一定的pointer信息，作者认为$h_t$和$e_o(a)$有以下关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_o(a)^Th_t = \left\{\begin{array}{l}
\ c\ \ \text{if}\ t\in R(a,p)\\
\ 0\ \ \text{otherwise}
\end{array}\right.&lt;/script&gt;

&lt;p&gt;如果满足上述关系就会推出两种readers是等价的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathop{\text{argmax}}_a e_0(a)^To &amp;= \mathop{\text{argmax}}_a e_o(a)^T\sum_t \alpha_th_t\\
&amp;=\mathop{\text{argmax}}_a\sum_t\alpha_te_o(a)^Th_t = \mathop{\text{argmax}}_a \sum_{t\in R(a,p)}\alpha_t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;同时作者也用数据来证明了这种猜想：&lt;br /&gt;
&lt;img src=&quot;http://oddpnmpll.bkt.clouddn.com/2016-11-29-085545.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者还认为attention $\alpha_t$和匿名化后的ID顺序无关，两个具有不同ID顺序的相同文档，应该具有相同的attention，$q^Th_t=q^Th_t^{‘}$。因此认为$h_t$包含与ID相关的内容，也包含与ID无关的内容：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q^T(h_i+e_o(a))=q^Th_{i\cdot}&lt;/script&gt;

&lt;p&gt;也就是等价于$q^Te_o(a)=0$，同时作者也用数据来证明了：&lt;br /&gt;
&lt;img src=&quot;http://oddpnmpll.bkt.clouddn.com/2016-11-29-091617.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;5 数据集&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cs.nyu.edu/~kcho/DMQA/&quot;&gt;CNN &amp;amp; DailyMail&lt;/a&gt;  &lt;br /&gt;
论文：&lt;a href=&quot;https://arxiv.org/abs/1506.03340&quot;&gt;Teaching Machines to Read and Comprehend&lt;/a&gt;  &lt;br /&gt;
数据来自CNN和Daily Mail新闻，文章中高亮显示而且挖空的就是问题。为了防止使用外界知识，将命名实体都用ID替换，给出答案候选集。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://tticnlp.github.io/who_did_what/&quot;&gt;Who-did-What(WDW)&lt;/a&gt;  &lt;br /&gt;
论文：&lt;a href=&quot;http://aclweb.org/anthology/D/D16/D16-1241.pdf&quot;&gt;Who did What: A Large-Scale Person-Centered Cloze Dataset&lt;/a&gt;  &lt;br /&gt;
数据来自LDC English Gigaword newswire corpus。该数据集为了防止文章摘要被使用，每一个问题都从两个独立的文章中生成，一篇用来做Context，一篇用来挖空作为问题。该数据集为了不像CNN&amp;amp;DailyMail那样将实体匿名，所有的问题都是人名实体。而且使用了一些简单的baselines来筛选掉那些容易解决的问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Children’s Book Test(CBT)  &lt;br /&gt;
论文：&lt;a href=&quot;https://arxiv.org/abs/1511.02301&quot;&gt;The goldilocks principle: Reading childrens books with explicit memory representations&lt;/a&gt;  &lt;br /&gt;
数据来自一个儿童读物，每个问题都是从中挑选出21条连续的句子，将前20条作为Context，将第21条挖空作为问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD&lt;/a&gt;  &lt;br /&gt;
论文：&lt;a href=&quot;https://arxiv.org/abs/1606.05250&quot;&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://fb.ai/babi&quot;&gt;bAbI&lt;/a&gt;  &lt;br /&gt;
论文：&lt;a href=&quot;https://arxiv.org/abs/1502.05698&quot;&gt;Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-7&quot;&gt;6 相关论文&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Stanford Reader  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1606.02858&quot;&gt;A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Memory Networks  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1503.08895&quot;&gt;End-To-End Memory Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Attentive Networks  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1506.03340&quot;&gt;Teaching Machines to Read and Comprehend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Attention-Sum Reader  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1603.01547&quot;&gt;Text Understanding with the Attention Sum Reader Network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gated-Attention Reader  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1606.01549&quot;&gt;Gated-Attention Readers for Text Comprehension&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Attention-over-Attention Reader  &lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1607.04423&quot;&gt;Attention-over-Attention Neural Networks for Reading Comprehension&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-8&quot;&gt;三、简评&lt;/h2&gt;
&lt;p&gt;在我看来这是一篇很全面的综述，作者全面总结了最近出现的各种Readers，对开展机器阅读方面的研究有一个很好的参考。但是我很好奇为什么这里没有提到Dynamic Memory Networks，但是我觉得不好归类吧，毕竟Dynamic Memory Networks的Answers是通过RNN来decode而得来的。&lt;/p&gt;

</description>
        <pubDate>Tue, 29 Nov 2016 13:31:01 +0800</pubDate>
        <link>https://Shawn1993.github.io/2016/11/29/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/</link>
        <guid isPermaLink="true">https://Shawn1993.github.io/2016/11/29/Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers/</guid>
        
        <category>QA</category>
        
        
        <category>论文阅读</category>
        
      </item>
    
  </channel>
</rss>
